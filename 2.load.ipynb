{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified census_records: 797 rows\n",
      "Verified locations: 797 rows\n",
      "Verified persons: 797 rows\n",
      "Verified personal_attributes: 797 rows\n",
      "Verified occupations: 797 rows\n",
      "Verified families: 468 rows\n",
      "Verified relationships: 797 rows\n",
      "Verified property_status: 483 rows\n",
      "Verified marital_status: 797 rows\n",
      "\n",
      "All tables verified successfully!\n",
      "\n",
      "Schema verification complete. Ready to proceed with upload.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Schema Verification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def verify_schema_compatibility():\n",
    "    \"\"\"Verify that CSV files match PostgreSQL schema requirements\"\"\"\n",
    "    schema = {\n",
    "        'census_records': {\n",
    "            'required_columns': ['record_id', 'census_year', 'source_pk'],\n",
    "            'integer_columns': ['record_id', 'census_year', 'source_pk'],\n",
    "            'varchar_columns': ['ed', 'page_number']\n",
    "        },\n",
    "        'locations': {\n",
    "            'required_columns': ['location_id', 'record_id'],\n",
    "            'integer_columns': ['location_id', 'record_id'],\n",
    "            'varchar_columns': ['street_name', 'house_num', 'build_num', 'dwelling_number', 'family_number']\n",
    "        },\n",
    "        'persons': {\n",
    "            'required_columns': ['person_id', 'first_name', 'last_name'],\n",
    "            'varchar_columns': ['person_id', 'first_name', 'last_name']\n",
    "        },\n",
    "        'personal_attributes': {\n",
    "            'required_columns': ['attribute_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['attribute_id', 'record_id'],  # Removed 'age' from integer columns\n",
    "            'varchar_columns': ['person_id', 'sex', 'race', 'place_birth', 'age']  # Added 'age' to varchar columns\n",
    "        },\n",
    "        'occupations': {\n",
    "            'required_columns': ['occupation_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['occupation_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'work', 'business']\n",
    "        },\n",
    "        'families': {\n",
    "            'required_columns': ['family_id', 'record_id', 'location_id'],\n",
    "            'integer_columns': ['record_id', 'location_id'],\n",
    "            'varchar_columns': ['family_id', 'head_first_name', 'head_last_name']\n",
    "        },\n",
    "        'relationships': {\n",
    "            'required_columns': ['relationship_id', 'person_id', 'family_id', 'record_id'],\n",
    "            'integer_columns': ['relationship_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'family_id', 'relation_to_head']\n",
    "        },\n",
    "        'property_status': {\n",
    "            'required_columns': ['property_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['property_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'owned_rented']\n",
    "        },\n",
    "        'marital_status': {\n",
    "            'required_columns': ['marital_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['marital_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'marital_status']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    issues = []\n",
    "    tables = {}\n",
    "    \n",
    "    # Load and verify each table\n",
    "    for table_name, requirements in schema.items():\n",
    "        try:\n",
    "            # Load table with string type for specific columns\n",
    "            file_path = f'data/processed/{table_name}.csv'\n",
    "            \n",
    "            # Define dtype dictionary for pandas read_csv\n",
    "            dtypes = {}\n",
    "            for col in requirements.get('varchar_columns', []):\n",
    "                dtypes[col] = str\n",
    "            \n",
    "            # Read CSV with specified dtypes\n",
    "            df = pd.read_csv(file_path, dtype=dtypes)\n",
    "            \n",
    "            # Convert integer columns\n",
    "            for col in requirements.get('integer_columns', []):\n",
    "                if col in df.columns:\n",
    "                    try:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='raise').astype('Int64')\n",
    "                    except:\n",
    "                        issues.append(f\"{table_name}: Unable to convert {col} to integer\")\n",
    "            \n",
    "            tables[table_name] = df\n",
    "            \n",
    "            # Check required columns\n",
    "            missing_cols = set(requirements['required_columns']) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues.append(f\"{table_name}: Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            print(f\"Verified {table_name}: {len(df)} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues.append(f\"Error loading {table_name}: {str(e)}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nSchema verification issues found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"- {issue}\")\n",
    "        return False, tables\n",
    "    \n",
    "    print(\"\\nAll tables verified successfully!\")\n",
    "    return True, tables\n",
    "\n",
    "# Run verification and store results in global variables\n",
    "schema_valid, tables = verify_schema_compatibility()\n",
    "\n",
    "if not schema_valid:\n",
    "    raise ValueError(\"Please fix schema issues before proceeding with upload\")\n",
    "\n",
    "print(\"\\nSchema verification complete. Ready to proceed with upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define upload function\n",
    "def upload_to_supabase(table_name, df, batch_size=100):\n",
    "    \"\"\"Upload dataframe to Supabase table in batches with error handling\"\"\"\n",
    "    print(f\"Uploading {table_name}...\")\n",
    "    total_rows = len(df)\n",
    "    successful_uploads = 0\n",
    "    \n",
    "    # Clean NaN values before converting to records\n",
    "    df = df.replace({np.nan: None})\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        try:\n",
    "            data, count = supabase.table(table_name).insert(batch).execute()\n",
    "            successful_uploads += len(batch)\n",
    "            \n",
    "            # Print progress\n",
    "            progress = (i + len(batch)) / total_rows * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({successful_uploads}/{total_rows} rows)\")\n",
    "            time.sleep(0.1)  # Small delay to avoid rate limits\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error uploading batch starting at row {i}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            # Save failed batch to logs directory with timestamp\n",
    "            timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "            error_log_path = f'logs/failed_uploads/failed_{table_name}_batch_{i}_{timestamp}.json'\n",
    "            \n",
    "            error_data = {\n",
    "                'table': table_name,\n",
    "                'batch_start': i,\n",
    "                'error': str(e),\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'data': batch\n",
    "            }\n",
    "            \n",
    "            # Ensure the directory exists\n",
    "            os.makedirs('logs/failed_uploads', exist_ok=True)\n",
    "            \n",
    "            # Write the error log\n",
    "            with open(error_log_path, 'w') as f:\n",
    "                json.dump(error_data, f, indent=2, default=str)\n",
    "            \n",
    "            # If this is a foreign key violation, stop the upload\n",
    "            if 'violates foreign key constraint' in str(e):\n",
    "                raise ValueError(f\"Foreign key violation in {table_name}. Upload aborted.\")\n",
    "    \n",
    "    return successful_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data cleaning function\n",
    "def clean_table_data(df, table_name):\n",
    "    \"\"\"Clean table data before upload\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Replace NaN with None\n",
    "    df = df.replace({np.nan: None})\n",
    "    \n",
    "    # Table-specific cleaning\n",
    "    if table_name == 'locations':\n",
    "        # Rename house_number to house_num if it exists\n",
    "        if 'house_number' in df.columns:\n",
    "            df = df.rename(columns={'house_number': 'house_num'})\n",
    "    \n",
    "    elif table_name == 'personal_attributes':\n",
    "        # Rename birth_place to place_birth if it exists\n",
    "        if 'birth_place' in df.columns:\n",
    "            df = df.rename(columns={'birth_place': 'place_birth'})\n",
    "    \n",
    "    elif table_name == 'families':\n",
    "        # Remove census_year if it exists (not in schema)\n",
    "        if 'census_year' in df.columns:\n",
    "            df = df.drop('census_year', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supabase connection initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: Initialize Supabase Connection\n",
    "from supabase import create_client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env.local\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Get Supabase credentials from environment variables\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "    raise ValueError(\"Supabase credentials not found in .env.local file\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"Supabase connection initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing census_records...\n",
      "Uploading census_records...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing persons...\n",
      "Uploading persons...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing locations...\n",
      "Uploading locations...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing families...\n",
      "Uploading families...\n",
      "Progress: 21.37% (100/468 rows)\n",
      "Progress: 42.74% (200/468 rows)\n",
      "Progress: 64.10% (300/468 rows)\n",
      "Progress: 85.47% (400/468 rows)\n",
      "Progress: 100.00% (468/468 rows)\n",
      "\n",
      "Processing personal_attributes...\n",
      "Uploading personal_attributes...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing occupations...\n",
      "Uploading occupations...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing property_status...\n",
      "Uploading property_status...\n",
      "Progress: 20.70% (100/483 rows)\n",
      "Progress: 41.41% (200/483 rows)\n",
      "Progress: 62.11% (300/483 rows)\n",
      "Progress: 82.82% (400/483 rows)\n",
      "Progress: 100.00% (483/483 rows)\n",
      "\n",
      "Processing marital_status...\n",
      "Uploading marital_status...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing relationships...\n",
      "Uploading relationships...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Upload Summary:\n",
      "census_records: 797/797 rows uploaded\n",
      "persons: 797/797 rows uploaded\n",
      "locations: 797/797 rows uploaded\n",
      "families: 468/468 rows uploaded\n",
      "personal_attributes: 797/797 rows uploaded\n",
      "occupations: 797/797 rows uploaded\n",
      "property_status: 483/483 rows uploaded\n",
      "marital_status: 797/797 rows uploaded\n",
      "relationships: 797/797 rows uploaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Upload tables in correct order\n",
    "upload_order = [\n",
    "    'census_records',  # Base table - no dependencies\n",
    "    'persons',        # Base table - no dependencies\n",
    "    'locations',      # Depends on census_records\n",
    "    'families',       # Depends on census_records and locations\n",
    "    'personal_attributes',  # Depends on persons and census_records\n",
    "    'occupations',    # Depends on persons and census_records\n",
    "    'property_status',  # Depends on persons and census_records\n",
    "    'marital_status', # Depends on persons and census_records\n",
    "    'relationships'   # Depends on persons, families, and census_records (load last)\n",
    "]\n",
    "\n",
    "# Track upload statistics\n",
    "upload_stats = {}\n",
    "\n",
    "# Upload tables in order\n",
    "for table_name in upload_order:\n",
    "    if table_name in tables:\n",
    "        df = tables[table_name]\n",
    "        if 'created_at' in df.columns:\n",
    "            df = df.drop('created_at', axis=1)\n",
    "        \n",
    "        print(f\"\\nProcessing {table_name}...\")\n",
    "        try:\n",
    "            # Clean the data before upload\n",
    "            df = clean_table_data(df, table_name)\n",
    "            successful_rows = upload_to_supabase(table_name, df)\n",
    "            upload_stats[table_name] = {\n",
    "                'total_rows': len(df),\n",
    "                'uploaded_rows': successful_rows\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {table_name}: {str(e)}\")\n",
    "            print(\"Stopping upload process due to error.\")\n",
    "            break\n",
    "\n",
    "# Add verification summary\n",
    "print(\"\\nUpload Summary:\")\n",
    "for table_name, stats in upload_stats.items():\n",
    "    print(f\"{table_name}: {stats['uploaded_rows']}/{stats['total_rows']} rows uploaded\")\n",
    "    if stats['uploaded_rows'] != stats['total_rows']:\n",
    "        print(f\"WARNING: Missing rows in {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Row Count Validation:\n",
      "--------------------------------------------------\n",
      "census_records: 797 rows\n",
      "persons: 797 rows\n",
      "locations: 797 rows\n",
      "families: 468 rows\n",
      "personal_attributes: 797 rows\n",
      "occupations: 797 rows\n",
      "property_status: 483 rows\n",
      "marital_status: 797 rows\n",
      "relationships: 797 rows\n",
      "\n",
      "2. Foreign Key Validation:\n",
      "--------------------------------------------------\n",
      "✓ Locations record_ids valid\n",
      "✓ Families record_ids valid\n",
      "✓ Families location_ids valid\n",
      "✓ Relationships record_ids valid\n",
      "✓ Relationships family_ids valid\n",
      "\n",
      "3. Data Quality Checks:\n",
      "--------------------------------------------------\n",
      "\n",
      "census_records null counts:\n",
      "  ed: 774 nulls\n",
      "  page_number: 797 nulls\n",
      "\n",
      "locations null counts:\n",
      "  street_name: 420 nulls\n",
      "  house_num: 478 nulls\n",
      "  build_num: 502 nulls\n",
      "  dwelling_number: 23 nulls\n",
      "  family_number: 23 nulls\n",
      "\n",
      "personal_attributes null counts:\n",
      "  sex: 21 nulls\n",
      "  race: 23 nulls\n",
      "  age: 9 nulls\n",
      "\n",
      "occupations null counts:\n",
      "  work: 486 nulls\n",
      "  business: 586 nulls\n",
      "\n",
      "property_status null counts:\n",
      "  owned_rented: 403 nulls\n",
      "\n",
      "marital_status null counts:\n",
      "  marital_status: 1 nulls\n",
      "\n",
      "Data type validation:\n",
      "\n",
      "census_records data types:\n",
      "  record_id: int64\n",
      "  census_year: int64\n",
      "  source_pk: int64\n",
      "  ed: object\n",
      "  page_number: float64\n",
      "\n",
      "persons data types:\n",
      "  person_id: object\n",
      "  first_name: object\n",
      "  last_name: object\n",
      "\n",
      "locations data types:\n",
      "  location_id: int64\n",
      "  record_id: int64\n",
      "  street_name: object\n",
      "  house_num: float64\n",
      "  build_num: object\n",
      "  dwelling_number: float64\n",
      "  family_number: float64\n",
      "\n",
      "families data types:\n",
      "  family_id: object\n",
      "  record_id: int64\n",
      "  location_id: int64\n",
      "  head_first_name: object\n",
      "  head_last_name: object\n",
      "\n",
      "personal_attributes data types:\n",
      "  attribute_id: int64\n",
      "  person_id: object\n",
      "  record_id: int64\n",
      "  sex: object\n",
      "  race: object\n",
      "  age: float64\n",
      "  place_birth: object\n",
      "\n",
      "occupations data types:\n",
      "  occupation_id: int64\n",
      "  person_id: object\n",
      "  record_id: int64\n",
      "  work: object\n",
      "  business: object\n",
      "\n",
      "property_status data types:\n",
      "  property_id: int64\n",
      "  person_id: object\n",
      "  record_id: int64\n",
      "  owned_rented: object\n",
      "\n",
      "marital_status data types:\n",
      "  marital_id: int64\n",
      "  person_id: object\n",
      "  record_id: int64\n",
      "  marital_status: object\n",
      "\n",
      "relationships data types:\n",
      "  relationship_id: int64\n",
      "  person_id: object\n",
      "  family_id: object\n",
      "  record_id: int64\n",
      "  relation_to_head: object\n",
      "\n",
      "4. Record Linkage Summary:\n",
      "--------------------------------------------------\n",
      "\n",
      "Records by census year:\n",
      "  1900: 186 records\n",
      "  1920: 186 records\n",
      "  1930: 50 records\n",
      "  1940: 61 records\n",
      "  1950: 314 records\n",
      "\n",
      "Families by census year:\n",
      "  F1900: 50 families\n",
      "  F1920: 33 families\n",
      "  F1930: 10 families\n",
      "  F1940: 61 families\n",
      "  F1950: 314 families\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Final Data Validation\n",
    "def validate_final_data():\n",
    "    \"\"\"Perform final validation checks on all processed data\"\"\"\n",
    "    validation_results = {\n",
    "        'row_counts': {},\n",
    "        'foreign_keys': [],\n",
    "        'data_quality': []\n",
    "    }\n",
    "    \n",
    "    # Load all tables\n",
    "    tables = {}\n",
    "    upload_order = [\n",
    "        'census_records',\n",
    "        'persons',\n",
    "        'locations',\n",
    "        'families',\n",
    "        'personal_attributes',\n",
    "        'occupations',\n",
    "        'property_status',\n",
    "        'marital_status',\n",
    "        'relationships'\n",
    "    ]\n",
    "    \n",
    "    for table_name in upload_order:\n",
    "        file_path = f'data/processed/{table_name}.csv'\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            tables[table_name] = df\n",
    "            validation_results['row_counts'][table_name] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {table_name}: {str(e)}\")\n",
    "            return\n",
    "    \n",
    "    print(\"\\n1. Row Count Validation:\")\n",
    "    print(\"-\" * 50)\n",
    "    for table, count in validation_results['row_counts'].items():\n",
    "        print(f\"{table}: {count:,} rows\")\n",
    "    \n",
    "    print(\"\\n2. Foreign Key Validation:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check census_records foreign keys\n",
    "    record_ids = set(tables['census_records']['record_id'])\n",
    "    \n",
    "    # Check locations foreign keys\n",
    "    loc_records = set(tables['locations']['record_id'])\n",
    "    invalid_loc = loc_records - record_ids\n",
    "    if invalid_loc:\n",
    "        print(f\"Invalid record_ids in locations: {len(invalid_loc)} records\")\n",
    "    else:\n",
    "        print(\"✓ Locations record_ids valid\")\n",
    "    \n",
    "    # Check families foreign keys\n",
    "    fam_records = set(tables['families']['record_id'])\n",
    "    fam_locations = set(tables['families']['location_id'])\n",
    "    loc_ids = set(tables['locations']['location_id'])\n",
    "    \n",
    "    invalid_fam_rec = fam_records - record_ids\n",
    "    invalid_fam_loc = fam_locations - loc_ids\n",
    "    \n",
    "    if invalid_fam_rec:\n",
    "        print(f\"Invalid record_ids in families: {len(invalid_fam_rec)} records\")\n",
    "    else:\n",
    "        print(\"✓ Families record_ids valid\")\n",
    "        \n",
    "    if invalid_fam_loc:\n",
    "        print(f\"Invalid location_ids in families: {len(invalid_fam_loc)} records\")\n",
    "    else:\n",
    "        print(\"✓ Families location_ids valid\")\n",
    "    \n",
    "    # Check relationships foreign keys\n",
    "    rel_records = set(tables['relationships']['record_id'])\n",
    "    rel_families = set(tables['relationships']['family_id'])\n",
    "    fam_ids = set(tables['families']['family_id'])\n",
    "    \n",
    "    invalid_rel_rec = rel_records - record_ids\n",
    "    invalid_rel_fam = rel_families - fam_ids\n",
    "    \n",
    "    if invalid_rel_rec:\n",
    "        print(f\"Invalid record_ids in relationships: {len(invalid_rel_rec)} records\")\n",
    "    else:\n",
    "        print(\"✓ Relationships record_ids valid\")\n",
    "        \n",
    "    if invalid_rel_fam:\n",
    "        print(f\"Invalid family_ids in relationships: {len(invalid_rel_fam)} records\")\n",
    "    else:\n",
    "        print(\"✓ Relationships family_ids valid\")\n",
    "    \n",
    "    print(\"\\n3. Data Quality Checks:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check for null values in required fields\n",
    "    for table_name, df in tables.items():\n",
    "        null_counts = df.isnull().sum()\n",
    "        if null_counts.any():\n",
    "            print(f\"\\n{table_name} null counts:\")\n",
    "            for col, count in null_counts[null_counts > 0].items():\n",
    "                print(f\"  {col}: {count:,} nulls\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\nData type validation:\")\n",
    "    for table_name, df in tables.items():\n",
    "        print(f\"\\n{table_name} data types:\")\n",
    "        for col, dtype in df.dtypes.items():\n",
    "            print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    print(\"\\n4. Record Linkage Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Count records by census year\n",
    "    year_counts = tables['census_records']['census_year'].value_counts().sort_index()\n",
    "    print(\"\\nRecords by census year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count:,} records\")\n",
    "    \n",
    "    # Count families by census year\n",
    "    family_years = tables['families']['family_id'].str[:5].value_counts().sort_index()\n",
    "    print(\"\\nFamilies by census year:\")\n",
    "    for year_prefix, count in family_years.items():\n",
    "        print(f\"  {year_prefix}: {count:,} families\")\n",
    "\n",
    "# Run final validation\n",
    "validate_final_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lakeland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

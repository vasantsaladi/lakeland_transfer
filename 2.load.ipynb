{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified census_records: 797 rows\n",
      "Verified locations: 797 rows\n",
      "Verified persons: 797 rows\n",
      "Verified personal_attributes: 797 rows\n",
      "Verified occupations: 797 rows\n",
      "Verified families: 468 rows\n",
      "Verified relationships: 797 rows\n",
      "Verified property_status: 483 rows\n",
      "Verified marital_status: 797 rows\n",
      "\n",
      "All tables verified successfully!\n",
      "\n",
      "Schema verification complete. Ready to proceed with upload.\n"
     ]
    }
   ],
   "source": [
    "# Cell 0: Schema Verification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def verify_schema_compatibility():\n",
    "    \"\"\"Verify that CSV files match PostgreSQL schema requirements\"\"\"\n",
    "    schema = {\n",
    "        'census_records': {\n",
    "            'required_columns': ['record_id', 'census_year', 'source_pk'],\n",
    "            'integer_columns': ['record_id', 'census_year', 'source_pk'],\n",
    "            'varchar_columns': ['ed', 'page_number']\n",
    "        },\n",
    "        'locations': {\n",
    "            'required_columns': ['location_id', 'record_id'],\n",
    "            'integer_columns': ['location_id', 'record_id'],\n",
    "            'varchar_columns': ['street_name', 'house_num', 'build_num', 'dwelling_number', 'family_number']\n",
    "        },\n",
    "        'persons': {\n",
    "            'required_columns': ['person_id', 'first_name', 'last_name'],\n",
    "            'varchar_columns': ['person_id', 'first_name', 'last_name']\n",
    "        },\n",
    "        'personal_attributes': {\n",
    "            'required_columns': ['attribute_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['attribute_id', 'record_id'],  # Removed 'age' from integer columns\n",
    "            'varchar_columns': ['person_id', 'sex', 'race', 'place_birth', 'age']  # Added 'age' to varchar columns\n",
    "        },\n",
    "        'occupations': {\n",
    "            'required_columns': ['occupation_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['occupation_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'work', 'business']\n",
    "        },\n",
    "        'families': {\n",
    "            'required_columns': ['family_id', 'record_id', 'location_id'],\n",
    "            'integer_columns': ['record_id', 'location_id'],\n",
    "            'varchar_columns': ['family_id', 'head_first_name', 'head_last_name']\n",
    "        },\n",
    "        'relationships': {\n",
    "            'required_columns': ['relationship_id', 'person_id', 'family_id', 'record_id'],\n",
    "            'integer_columns': ['relationship_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'family_id', 'relation_to_head']\n",
    "        },\n",
    "        'property_status': {\n",
    "            'required_columns': ['property_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['property_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'owned_rented']\n",
    "        },\n",
    "        'marital_status': {\n",
    "            'required_columns': ['marital_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['marital_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'marital_status']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    issues = []\n",
    "    tables = {}\n",
    "    \n",
    "    # Load and verify each table\n",
    "    for table_name, requirements in schema.items():\n",
    "        try:\n",
    "            # Load table with string type for specific columns\n",
    "            file_path = f'data/processed/{table_name}.csv'\n",
    "            \n",
    "            # Define dtype dictionary for pandas read_csv\n",
    "            dtypes = {}\n",
    "            for col in requirements.get('varchar_columns', []):\n",
    "                dtypes[col] = str\n",
    "            \n",
    "            # Read CSV with specified dtypes\n",
    "            df = pd.read_csv(file_path, dtype=dtypes)\n",
    "            \n",
    "            # Convert integer columns\n",
    "            for col in requirements.get('integer_columns', []):\n",
    "                if col in df.columns:\n",
    "                    try:\n",
    "                        df[col] = pd.to_numeric(df[col], errors='raise').astype('Int64')\n",
    "                    except:\n",
    "                        issues.append(f\"{table_name}: Unable to convert {col} to integer\")\n",
    "            \n",
    "            tables[table_name] = df\n",
    "            \n",
    "            # Check required columns\n",
    "            missing_cols = set(requirements['required_columns']) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues.append(f\"{table_name}: Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            print(f\"Verified {table_name}: {len(df)} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues.append(f\"Error loading {table_name}: {str(e)}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nSchema verification issues found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"- {issue}\")\n",
    "        return False, tables\n",
    "    \n",
    "    print(\"\\nAll tables verified successfully!\")\n",
    "    return True, tables\n",
    "\n",
    "# Run verification and store results in global variables\n",
    "schema_valid, tables = verify_schema_compatibility()\n",
    "\n",
    "if not schema_valid:\n",
    "    raise ValueError(\"Please fix schema issues before proceeding with upload\")\n",
    "\n",
    "print(\"\\nSchema verification complete. Ready to proceed with upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define upload function\n",
    "def upload_to_supabase(table_name, df, batch_size=100):\n",
    "    \"\"\"Upload dataframe to Supabase table in batches with error handling\"\"\"\n",
    "    print(f\"Uploading {table_name}...\")\n",
    "    total_rows = len(df)\n",
    "    successful_uploads = 0\n",
    "    \n",
    "    # Clean NaN values before converting to records\n",
    "    df = df.replace({np.nan: None})\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        try:\n",
    "            data, count = supabase.table(table_name).insert(batch).execute()\n",
    "            successful_uploads += len(batch)\n",
    "            \n",
    "            # Print progress\n",
    "            progress = (i + len(batch)) / total_rows * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({successful_uploads}/{total_rows} rows)\")\n",
    "            time.sleep(0.1)  # Small delay to avoid rate limits\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error uploading batch starting at row {i}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            # Save failed batch to logs directory with timestamp\n",
    "            timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "            error_log_path = f'logs/failed_uploads/failed_{table_name}_batch_{i}_{timestamp}.json'\n",
    "            \n",
    "            error_data = {\n",
    "                'table': table_name,\n",
    "                'batch_start': i,\n",
    "                'error': str(e),\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'data': batch\n",
    "            }\n",
    "            \n",
    "            # Ensure the directory exists\n",
    "            os.makedirs('logs/failed_uploads', exist_ok=True)\n",
    "            \n",
    "            # Write the error log\n",
    "            with open(error_log_path, 'w') as f:\n",
    "                json.dump(error_data, f, indent=2, default=str)\n",
    "            \n",
    "            # If this is a foreign key violation, stop the upload\n",
    "            if 'violates foreign key constraint' in str(e):\n",
    "                raise ValueError(f\"Foreign key violation in {table_name}. Upload aborted.\")\n",
    "    \n",
    "    return successful_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data cleaning function\n",
    "def clean_table_data(df, table_name):\n",
    "    \"\"\"Clean table data before upload\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Replace NaN with None\n",
    "    df = df.replace({np.nan: None})\n",
    "    \n",
    "    # Table-specific cleaning\n",
    "    if table_name == 'locations':\n",
    "        # Rename house_number to house_num if it exists\n",
    "        if 'house_number' in df.columns:\n",
    "            df = df.rename(columns={'house_number': 'house_num'})\n",
    "    \n",
    "    elif table_name == 'personal_attributes':\n",
    "        # Rename birth_place to place_birth if it exists\n",
    "        if 'birth_place' in df.columns:\n",
    "            df = df.rename(columns={'birth_place': 'place_birth'})\n",
    "    \n",
    "    elif table_name == 'families':\n",
    "        # Remove census_year if it exists (not in schema)\n",
    "        if 'census_year' in df.columns:\n",
    "            df = df.drop('census_year', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supabase connection initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 3.5: Initialize Supabase Connection\n",
    "from supabase import create_client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "\n",
    "# Load environment variables from .env.local\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Get Supabase credentials from environment variables\n",
    "SUPABASE_URL = os.getenv('SUPABASE_URL')\n",
    "SUPABASE_KEY = os.getenv('SUPABASE_KEY')\n",
    "\n",
    "if not SUPABASE_URL or not SUPABASE_KEY:\n",
    "    raise ValueError(\"Supabase credentials not found in .env.local file\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "print(\"Supabase connection initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing census_records...\n",
      "Uploading census_records...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing persons...\n",
      "Uploading persons...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing locations...\n",
      "Uploading locations...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing families...\n",
      "Uploading families...\n",
      "Progress: 21.37% (100/468 rows)\n",
      "Progress: 42.74% (200/468 rows)\n",
      "Progress: 64.10% (300/468 rows)\n",
      "Progress: 85.47% (400/468 rows)\n",
      "Progress: 100.00% (468/468 rows)\n",
      "\n",
      "Processing personal_attributes...\n",
      "Uploading personal_attributes...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing occupations...\n",
      "Uploading occupations...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing property_status...\n",
      "Uploading property_status...\n",
      "Progress: 20.70% (100/483 rows)\n",
      "Progress: 41.41% (200/483 rows)\n",
      "Progress: 62.11% (300/483 rows)\n",
      "Progress: 82.82% (400/483 rows)\n",
      "Progress: 100.00% (483/483 rows)\n",
      "\n",
      "Processing marital_status...\n",
      "Uploading marital_status...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Processing relationships...\n",
      "Uploading relationships...\n",
      "Progress: 12.55% (100/797 rows)\n",
      "Progress: 25.09% (200/797 rows)\n",
      "Progress: 37.64% (300/797 rows)\n",
      "Progress: 50.19% (400/797 rows)\n",
      "Progress: 62.74% (500/797 rows)\n",
      "Progress: 75.28% (600/797 rows)\n",
      "Progress: 87.83% (700/797 rows)\n",
      "Progress: 100.00% (797/797 rows)\n",
      "\n",
      "Upload Summary:\n",
      "census_records: 797/797 rows uploaded\n",
      "persons: 797/797 rows uploaded\n",
      "locations: 797/797 rows uploaded\n",
      "families: 468/468 rows uploaded\n",
      "personal_attributes: 797/797 rows uploaded\n",
      "occupations: 797/797 rows uploaded\n",
      "property_status: 483/483 rows uploaded\n",
      "marital_status: 797/797 rows uploaded\n",
      "relationships: 797/797 rows uploaded\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Upload tables in correct order\n",
    "upload_order = [\n",
    "    'census_records',  # Base table - no dependencies\n",
    "    'persons',        # Base table - no dependencies\n",
    "    'locations',      # Depends on census_records\n",
    "    'families',       # Depends on census_records and locations\n",
    "    'personal_attributes',  # Depends on persons and census_records\n",
    "    'occupations',    # Depends on persons and census_records\n",
    "    'property_status',  # Depends on persons and census_records\n",
    "    'marital_status', # Depends on persons and census_records\n",
    "    'relationships'   # Depends on persons, families, and census_records (load last)\n",
    "]\n",
    "\n",
    "# Track upload statistics\n",
    "upload_stats = {}\n",
    "\n",
    "# Upload tables in order\n",
    "for table_name in upload_order:\n",
    "    if table_name in tables:\n",
    "        df = tables[table_name]\n",
    "        if 'created_at' in df.columns:\n",
    "            df = df.drop('created_at', axis=1)\n",
    "        \n",
    "        print(f\"\\nProcessing {table_name}...\")\n",
    "        try:\n",
    "            # Clean the data before upload\n",
    "            df = clean_table_data(df, table_name)\n",
    "            successful_rows = upload_to_supabase(table_name, df)\n",
    "            upload_stats[table_name] = {\n",
    "                'total_rows': len(df),\n",
    "                'uploaded_rows': successful_rows\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {table_name}: {str(e)}\")\n",
    "            print(\"Stopping upload process due to error.\")\n",
    "            break\n",
    "\n",
    "# Add verification summary\n",
    "print(\"\\nUpload Summary:\")\n",
    "for table_name, stats in upload_stats.items():\n",
    "    print(f\"{table_name}: {stats['uploaded_rows']}/{stats['total_rows']} rows uploaded\")\n",
    "    if stats['uploaded_rows'] != stats['total_rows']:\n",
    "        print(f\"WARNING: Missing rows in {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lakeland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

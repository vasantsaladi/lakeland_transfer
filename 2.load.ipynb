{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified census_records: 797 rows\n",
      "Verified locations: 797 rows\n",
      "Verified persons: 797 rows\n",
      "Verified personal_attributes: 797 rows\n",
      "Verified occupations: 797 rows\n",
      "Verified families: 468 rows\n",
      "Verified relationships: 797 rows\n",
      "Verified property_status: 483 rows\n",
      "Verified marital_status: 797 rows\n",
      "\n",
      "Schema verification issues found:\n",
      "- locations: Column house_num should be string type\n",
      "- locations: Column dwelling_number should be string type\n",
      "- locations: Column family_number should be string type\n",
      "- personal_attributes: Non-integer values in age\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Please fix schema issues before proceeding with upload",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m schema_valid, verified_tables \u001b[38;5;241m=\u001b[39m verify_schema_compatibility()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m schema_valid:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease fix schema issues before proceeding with upload\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSchema verification complete. Ready to proceed with upload.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Please fix schema issues before proceeding with upload"
     ]
    }
   ],
   "source": [
    "# Cell 0: Schema Verification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def verify_schema_compatibility():\n",
    "    \"\"\"Verify that CSV files match PostgreSQL schema requirements\"\"\"\n",
    "    schema = {\n",
    "        'census_records': {\n",
    "            'required_columns': ['record_id', 'census_year', 'source_pk'],\n",
    "            'integer_columns': ['record_id', 'census_year', 'source_pk'],\n",
    "            'varchar_columns': ['ed', 'page_number']\n",
    "        },\n",
    "        'locations': {\n",
    "            'required_columns': ['location_id', 'record_id'],\n",
    "            'integer_columns': ['location_id', 'record_id'],\n",
    "            'varchar_columns': ['street_name', 'house_num', 'build_num', 'dwelling_number', 'family_number']\n",
    "        },\n",
    "        'persons': {\n",
    "            'required_columns': ['person_id', 'first_name', 'last_name'],\n",
    "            'varchar_columns': ['person_id', 'first_name', 'last_name']\n",
    "        },\n",
    "        'personal_attributes': {\n",
    "            'required_columns': ['attribute_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['attribute_id', 'record_id', 'age'],\n",
    "            'varchar_columns': ['person_id', 'sex', 'race', 'place_birth']\n",
    "        },\n",
    "        'occupations': {\n",
    "            'required_columns': ['occupation_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['occupation_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'work', 'business']\n",
    "        },\n",
    "        'families': {\n",
    "            'required_columns': ['family_id', 'record_id', 'location_id'],\n",
    "            'integer_columns': ['record_id', 'location_id'],\n",
    "            'varchar_columns': ['family_id', 'head_first_name', 'head_last_name']\n",
    "        },\n",
    "        'relationships': {\n",
    "            'required_columns': ['relationship_id', 'person_id', 'family_id', 'record_id'],\n",
    "            'integer_columns': ['relationship_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'family_id', 'relation_to_head']\n",
    "        },\n",
    "        'property_status': {\n",
    "            'required_columns': ['property_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['property_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'owned_rented']\n",
    "        },\n",
    "        'marital_status': {\n",
    "            'required_columns': ['marital_id', 'person_id', 'record_id'],\n",
    "            'integer_columns': ['marital_id', 'record_id'],\n",
    "            'varchar_columns': ['person_id', 'marital_status']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    issues = []\n",
    "    tables = {}\n",
    "    \n",
    "    # Load and verify each table\n",
    "    for table_name, requirements in schema.items():\n",
    "        try:\n",
    "            # Load table\n",
    "            file_path = f'data/processed/{table_name}.csv'\n",
    "            df = pd.read_csv(file_path)\n",
    "            tables[table_name] = df\n",
    "            \n",
    "            # Check required columns\n",
    "            missing_cols = set(requirements['required_columns']) - set(df.columns)\n",
    "            if missing_cols:\n",
    "                issues.append(f\"{table_name}: Missing required columns: {missing_cols}\")\n",
    "            \n",
    "            # Check integer columns\n",
    "            for col in requirements.get('integer_columns', []):\n",
    "                if col in df.columns:\n",
    "                    non_int_mask = ~df[col].isna() & ~df[col].astype(str).str.match(r'^\\d+$')\n",
    "                    if non_int_mask.any():\n",
    "                        issues.append(f\"{table_name}: Non-integer values in {col}\")\n",
    "            \n",
    "            # Check varchar columns\n",
    "            for col in requirements.get('varchar_columns', []):\n",
    "                if col in df.columns and df[col].notna().any():\n",
    "                    if not df[col].dtype == object:\n",
    "                        issues.append(f\"{table_name}: Column {col} should be string type\")\n",
    "            \n",
    "            print(f\"Verified {table_name}: {len(df)} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            issues.append(f\"Error loading {table_name}: {str(e)}\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"\\nSchema verification issues found:\")\n",
    "        for issue in issues:\n",
    "            print(f\"- {issue}\")\n",
    "        return False, tables\n",
    "    \n",
    "    print(\"\\nAll tables verified successfully!\")\n",
    "    return True, tables\n",
    "\n",
    "# Verify schema compatibility\n",
    "schema_valid, verified_tables = verify_schema_compatibility()\n",
    "\n",
    "if not schema_valid:\n",
    "    raise ValueError(\"Please fix schema issues before proceeding with upload\")\n",
    "\n",
    "print(\"\\nSchema verification complete. Ready to proceed with upload.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define upload function\n",
    "def upload_to_supabase(table_name, df, batch_size=100):\n",
    "    \"\"\"Upload dataframe to Supabase table in batches with error handling\"\"\"\n",
    "    print(f\"Uploading {table_name}...\")\n",
    "    total_rows = len(df)\n",
    "    successful_uploads = 0\n",
    "    \n",
    "    # Clean NaN values before converting to records\n",
    "    df = df.replace({np.nan: None})\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        try:\n",
    "            data, count = supabase.table(table_name).insert(batch).execute()\n",
    "            successful_uploads += len(batch)\n",
    "            \n",
    "            # Print progress\n",
    "            progress = (i + len(batch)) / total_rows * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({successful_uploads}/{total_rows} rows)\")\n",
    "            time.sleep(0.1)  # Small delay to avoid rate limits\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error uploading batch starting at row {i}: {str(e)}\"\n",
    "            print(error_msg)\n",
    "            \n",
    "            # Save failed batch to logs directory with timestamp\n",
    "            timestamp = time.strftime('%Y%m%d_%H%M%S')\n",
    "            error_log_path = f'logs/failed_uploads/failed_{table_name}_batch_{i}_{timestamp}.json'\n",
    "            \n",
    "            error_data = {\n",
    "                'table': table_name,\n",
    "                'batch_start': i,\n",
    "                'error': str(e),\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'data': batch\n",
    "            }\n",
    "            \n",
    "            # Ensure the directory exists\n",
    "            os.makedirs('logs/failed_uploads', exist_ok=True)\n",
    "            \n",
    "            # Write the error log\n",
    "            with open(error_log_path, 'w') as f:\n",
    "                json.dump(error_data, f, indent=2, default=str)\n",
    "            \n",
    "            # If this is a foreign key violation, stop the upload\n",
    "            if 'violates foreign key constraint' in str(e):\n",
    "                raise ValueError(f\"Foreign key violation in {table_name}. Upload aborted.\")\n",
    "    \n",
    "    return successful_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data cleaning function\n",
    "def clean_table_data(df, table_name):\n",
    "    \"\"\"Clean table data before upload\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Replace NaN with None\n",
    "    df = df.replace({np.nan: None})\n",
    "    \n",
    "    # Table-specific cleaning\n",
    "    if table_name == 'locations':\n",
    "        # Rename house_number to house_num if it exists\n",
    "        if 'house_number' in df.columns:\n",
    "            df = df.rename(columns={'house_number': 'house_num'})\n",
    "    \n",
    "    elif table_name == 'personal_attributes':\n",
    "        # Rename birth_place to place_birth if it exists\n",
    "        if 'birth_place' in df.columns:\n",
    "            df = df.rename(columns={'birth_place': 'place_birth'})\n",
    "    \n",
    "    elif table_name == 'families':\n",
    "        # Remove census_year if it exists (not in schema)\n",
    "        if 'census_year' in df.columns:\n",
    "            df = df.drop('census_year', axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Upload tables in order\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_name \u001b[38;5;129;01min\u001b[39;00m upload_order:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m table_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtables\u001b[49m:\n\u001b[1;32m     20\u001b[0m         df \u001b[38;5;241m=\u001b[39m tables[table_name]\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated_at\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tables' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 4: Upload tables in correct order\n",
    "upload_order = [\n",
    "    'census_records',  # Base table - no dependencies\n",
    "    'persons',        # Base table - no dependencies\n",
    "    'locations',      # Depends on census_records\n",
    "    'families',       # Depends on census_records and locations\n",
    "    'personal_attributes',  # Depends on persons and census_records\n",
    "    'occupations',    # Depends on persons and census_records\n",
    "    'property_status',  # Depends on persons and census_records\n",
    "    'marital_status', # Depends on persons and census_records\n",
    "    'relationships'   # Depends on persons, families, and census_records (load last)\n",
    "]\n",
    "\n",
    "# Track upload statistics\n",
    "upload_stats = {}\n",
    "\n",
    "# Upload tables in order\n",
    "for table_name in upload_order:\n",
    "    if table_name in tables:\n",
    "        df = tables[table_name]\n",
    "        if 'created_at' in df.columns:\n",
    "            df = df.drop('created_at', axis=1)\n",
    "        \n",
    "        print(f\"\\nProcessing {table_name}...\")\n",
    "        try:\n",
    "            # Clean the data before upload\n",
    "            df = clean_table_data(df, table_name)\n",
    "            successful_rows = upload_to_supabase(table_name, df)\n",
    "            upload_stats[table_name] = {\n",
    "                'total_rows': len(df),\n",
    "                'uploaded_rows': successful_rows\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload {table_name}: {str(e)}\")\n",
    "            print(\"Stopping upload process due to error.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lakeland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Supabase\n",
      "Successfully loaded census_records\n",
      "Successfully loaded locations\n",
      "Successfully loaded persons\n",
      "Successfully loaded personal_attributes\n",
      "Successfully loaded occupations\n",
      "Successfully loaded families\n",
      "Successfully loaded relationships\n",
      "Successfully loaded property_status\n",
      "Successfully loaded marital_status\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from supabase import create_client\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Create directories for processed data and error logs\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('logs/failed_uploads', exist_ok=True)\n",
    "\n",
    "# Add to .gitignore if it doesn't exist\n",
    "gitignore_path = '.gitignore'\n",
    "gitignore_entries = \"\"\"\n",
    "# Processed data and logs\n",
    "data/processed/\n",
    "logs/\n",
    ".env.local\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists(gitignore_path):\n",
    "    with open(gitignore_path, 'w') as f:\n",
    "        f.write(gitignore_entries)\n",
    "else:\n",
    "    with open(gitignore_path, 'a') as f:\n",
    "        f.write(gitignore_entries)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('.env.local')\n",
    "url = os.environ.get(\"SUPABASE_URL\")\n",
    "key = os.environ.get(\"SUPABASE_KEY\")\n",
    "\n",
    "if not url or not key:\n",
    "    raise ValueError(\"Missing Supabase credentials. Please check .env.local file\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "try:\n",
    "    supabase = create_client(url, key)\n",
    "    print(\"Successfully connected to Supabase\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to Supabase: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Read processed CSV files with error handling\n",
    "tables = {}\n",
    "required_tables = [\n",
    "    'census_records', 'locations', 'persons', 'personal_attributes',\n",
    "    'occupations', 'families', 'relationships', 'property_status',\n",
    "    'marital_status'\n",
    "]\n",
    "\n",
    "for table_name in required_tables:\n",
    "    file_path = f'data/processed/{table_name}.csv'\n",
    "    try:\n",
    "        tables[table_name] = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded {table_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {table_name}: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define upload function\n",
    "def upload_to_supabase(table_name, df, batch_size=100):\n",
    "    \"\"\"Upload dataframe to Supabase table in batches with error handling\"\"\"\n",
    "    print(f\"Uploading {table_name}...\")\n",
    "    total_rows = len(df)\n",
    "    successful_uploads = 0\n",
    "    \n",
    "    # Convert DataFrame to list of dictionaries\n",
    "    records = df.to_dict('records')\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = records[i:i + batch_size]\n",
    "        try:\n",
    "            data, count = supabase.table(table_name).insert(batch).execute()\n",
    "            successful_uploads += len(batch)\n",
    "            \n",
    "            # Print progress\n",
    "            progress = (i + len(batch)) / total_rows * 100\n",
    "            print(f\"Progress: {progress:.2f}% ({successful_uploads}/{total_rows} rows)\")\n",
    "            time.sleep(0.1)  # Small delay to avoid rate limits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading batch starting at row {i}: {str(e)}\")\n",
    "            with open(f'failed_{table_name}_batch_{i}.txt', 'w') as f:\n",
    "                f.write(str(batch))\n",
    "    \n",
    "    return successful_uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing census_records...\n",
      "Uploading census_records...\n",
      "Error uploading batch starting at row 0: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 100: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 200: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 300: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 400: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 500: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 600: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 700: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "\n",
      "Processing locations...\n",
      "Uploading locations...\n",
      "Error uploading batch starting at row 0: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 100: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 200: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 300: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 400: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 500: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 600: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 700: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "\n",
      "Processing persons...\n",
      "Uploading persons...\n",
      "Error uploading batch starting at row 0: {'code': '23505', 'details': 'Key (person_id)=(P1900_000000) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "Error uploading batch starting at row 100: {'code': '23505', 'details': 'Key (person_id)=(P1900_000100) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "Error uploading batch starting at row 200: {'code': '23505', 'details': 'Key (person_id)=(P1920_000014) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "Error uploading batch starting at row 300: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 400: {'code': '23505', 'details': 'Key (person_id)=(P1930_000028) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "Error uploading batch starting at row 500: {'code': '23505', 'details': 'Key (person_id)=(P1950_000017) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "Error uploading batch starting at row 600: {'code': '23505', 'details': 'Key (person_id)=(P1950_000117) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "Error uploading batch starting at row 700: {'code': '23505', 'details': 'Key (person_id)=(P1950_000217) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"persons_pkey\"'}\n",
      "\n",
      "Processing families...\n",
      "Uploading families...\n",
      "Error uploading batch starting at row 0: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 100: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 200: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 300: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 400: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 500: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 600: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "Error uploading batch starting at row 700: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'census_year' column of 'families' in the schema cache\"}\n",
      "\n",
      "Processing personal_attributes...\n",
      "Uploading personal_attributes...\n",
      "Error uploading batch starting at row 0: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 100: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 200: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 300: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 400: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 500: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 600: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "Error uploading batch starting at row 700: {'code': 'PGRST204', 'details': None, 'hint': None, 'message': \"Could not find the 'birth_place' column of 'personal_attributes' in the schema cache\"}\n",
      "\n",
      "Processing occupations...\n",
      "Uploading occupations...\n",
      "Error uploading batch starting at row 0: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 100: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 200: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 300: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 400: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 500: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 600: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 700: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "\n",
      "Processing relationships...\n",
      "Uploading relationships...\n",
      "Error uploading batch starting at row 0: {'code': '23503', 'details': 'Key (family_id)=(F1900_000000) is not present in table \"families\".', 'hint': None, 'message': 'insert or update on table \"relationships\" violates foreign key constraint \"relationships_family_id_fkey\"'}\n",
      "Error uploading batch starting at row 100: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 200: {'code': '23503', 'details': 'Key (family_id)=(F1920_000014) is not present in table \"families\".', 'hint': None, 'message': 'insert or update on table \"relationships\" violates foreign key constraint \"relationships_family_id_fkey\"'}\n",
      "Error uploading batch starting at row 300: {'code': '23503', 'details': 'Key (person_id)=(P1920_000114) is not present in table \"persons\".', 'hint': None, 'message': 'insert or update on table \"relationships\" violates foreign key constraint \"relationships_person_id_fkey\"'}\n",
      "Error uploading batch starting at row 400: {'code': '23505', 'details': 'Key (relationship_id)=(29) already exists.', 'hint': None, 'message': 'duplicate key value violates unique constraint \"relationships_pkey\"'}\n",
      "Error uploading batch starting at row 500: {'code': '23503', 'details': 'Key (family_id)=(F1950_000017) is not present in table \"families\".', 'hint': None, 'message': 'insert or update on table \"relationships\" violates foreign key constraint \"relationships_family_id_fkey\"'}\n",
      "Error uploading batch starting at row 600: {'code': '23503', 'details': 'Key (family_id)=(F1950_000117) is not present in table \"families\".', 'hint': None, 'message': 'insert or update on table \"relationships\" violates foreign key constraint \"relationships_family_id_fkey\"'}\n",
      "Error uploading batch starting at row 700: {'code': '23503', 'details': 'Key (family_id)=(F1950_000217) is not present in table \"families\".', 'hint': None, 'message': 'insert or update on table \"relationships\" violates foreign key constraint \"relationships_family_id_fkey\"'}\n",
      "\n",
      "Processing property_status...\n",
      "Uploading property_status...\n",
      "Error uploading batch starting at row 0: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 100: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 200: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 300: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 400: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "\n",
      "Processing marital_status...\n",
      "Uploading marital_status...\n",
      "Error uploading batch starting at row 0: {'code': '23503', 'details': 'Key (record_id)=(0) is not present in table \"census_records\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_record_id_fkey\"'}\n",
      "Error uploading batch starting at row 100: {'code': '23503', 'details': 'Key (record_id)=(100) is not present in table \"census_records\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_record_id_fkey\"'}\n",
      "Error uploading batch starting at row 200: {'code': '23503', 'details': 'Key (record_id)=(14) is not present in table \"census_records\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_record_id_fkey\"'}\n",
      "Error uploading batch starting at row 300: {'code': '23503', 'details': 'Key (person_id)=(P1920_000114) is not present in table \"persons\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_person_id_fkey\"'}\n",
      "Error uploading batch starting at row 400: {'code': '22P02', 'details': 'Token \"NaN\" is invalid.', 'hint': None, 'message': 'invalid input syntax for type json'}\n",
      "Error uploading batch starting at row 500: {'code': '23503', 'details': 'Key (record_id)=(17) is not present in table \"census_records\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_record_id_fkey\"'}\n",
      "Error uploading batch starting at row 600: {'code': '23503', 'details': 'Key (record_id)=(117) is not present in table \"census_records\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_record_id_fkey\"'}\n",
      "Error uploading batch starting at row 700: {'code': '23503', 'details': 'Key (record_id)=(217) is not present in table \"census_records\".', 'hint': None, 'message': 'insert or update on table \"marital_status\" violates foreign key constraint \"marital_status_record_id_fkey\"'}\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Upload tables in correct order\n",
    "upload_order = [\n",
    "    'census_records',  # No foreign keys\n",
    "    'locations',       # Depends on census_records\n",
    "    'persons',        # No foreign keys\n",
    "    'families',       # Depends on census_records and locations\n",
    "    'personal_attributes',  # Depends on persons and census_records\n",
    "    'occupations',    # Depends on persons and census_records\n",
    "    'relationships',  # Depends on persons, families, and census_records\n",
    "    'property_status',  # Depends on persons and census_records\n",
    "    'marital_status'   # Depends on persons and census_records\n",
    "]\n",
    "\n",
    "# Track upload statistics\n",
    "upload_stats = {}\n",
    "\n",
    "# Upload tables in order\n",
    "for table_name in upload_order:\n",
    "    if table_name in tables:\n",
    "        df = tables[table_name]\n",
    "        if 'created_at' in df.columns:\n",
    "            df = df.drop('created_at', axis=1)\n",
    "        \n",
    "        print(f\"\\nProcessing {table_name}...\")\n",
    "        successful_rows = upload_to_supabase(table_name, df)\n",
    "        upload_stats[table_name] = {\n",
    "            'total_rows': len(df),\n",
    "            'uploaded_rows': successful_rows\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Upload Summary:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cell 4: Print upload summary\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mUpload Summary:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table_name, stats \u001b[38;5;129;01min\u001b[39;00m upload_stats\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# Cell 4: Print upload summary\n",
    "2|print(\"\\nUpload Summary:\")\n",
    "print(\"-\" * 50)\n",
    "for table_name, stats in upload_stats.items():\n",
    "    success_rate = (stats['uploaded_rows'] / stats['total_rows']) * 100\n",
    "    print(f\"{table_name}:\")\n",
    "    print(f\"  Total rows: {stats['total_rows']}\")\n",
    "    print(f\"  Uploaded rows: {stats['uploaded_rows']}\")\n",
    "    print(f\"  Success rate: {success_rate:.2f}%\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lakeland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
